In our experiment, we assess the performance of the proposed algorithm on the partially-observed diabolical combination lock (pocomblock) problem, characterized by a horizon $H$ and a set of 10 actions. At each temporal stage $h$, there exist three latent states $s_{i;h}$ for $i \in {0, 1, 2}$. We denote the states $s_{i;h}$ for $i \in {0, 1}$ as favorable states and $s_{2;h}$ as unfavorable states. For each $s_{i;h}$ with $i \in {0, 1}$, a specific action $a_{i;h}^*$ is randomly selected from the 10 available actions.

When the agent is in state $s_{i;h}$ for $i \in {0, 1}$ and performs action $a_{i;h}^*$, it transitions to states $s_{0;h+1}$ and $s_{1;h+1}$ with equal likelihood. Conversely, executing any other actions will deterministically lead the agent to state $s_{2;h+1}$. In the unfavorable state $s_{2;h}$, any action taken by the agent will inevitably result in transitioning to state $s_{2;h+1}$.

As for the reward function, a reward of 1 is assigned to states $s_{i;H}$ for $i \in {0, 1}$, signifying that favorable states at horizon $H$ yield a reward of 1. Additionally, with a 0.5 probability, the agent receives an anti-shaped reward of 0.1 upon transitioning from a favorable state to an unfavorable state. All other states and transitions yield a reward of zero.

When the step $h$ is odd, the observation $o$ is generated with a dimension of $2^{\lceil\log(H+4)\rceil}$ by concatenating the one-hot vectors of latent state $s$ and horizon $h$, introducing Gaussian noise sampled from $\mathcal{N}(0,0.1)$ for each dimension, appending 0 if needed, and multiplying with a Hadamard matrix. For even steps $h$, the observations corresponding to the one of the good states and the bad states are identical, the other good state's observation function is same as the time step is odd. The initial state distribution is uniformly distributed across $s_{i;0}$ for $i \in {0, 1}$. We employ a two-layer neural network to capture the essential features of the problem.

It is noteworthy that the optimal policy consists of selecting the specific action $a_{i;h}^*$ at each step $h$. Once the agent enters an unfavorable state, it remains trapped in unfavorable states for the entire episode, thus failing to attain the substantial reward signal at the conclusion. This presents an exceptionally demanding exploration problem, as a uniform random policy offers a mere $10^{-H}$ probability of achieving the objectives.
